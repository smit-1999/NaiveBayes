{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3845e91-336d-4703-a0b8-0df012ccd064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefd3217-edc9-4691-85c0-3eca37c1ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(imgf, labelf, outf, n):\n",
    "    f = open(imgf, \"rb\")\n",
    "    o = open(outf, \"w\")\n",
    "    l = open(labelf, \"rb\")\n",
    "\n",
    "    f.read(16)\n",
    "    l.read(8)\n",
    "    images = []\n",
    "\n",
    "    for i in range(n+1):\n",
    "        image = [ord(l.read(1))]\n",
    "        for j in range(28*28):\n",
    "            image.append(ord(f.read(1)))\n",
    "        images.append(image)\n",
    "\n",
    "    for image in images:\n",
    "        o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "    f.close()\n",
    "    o.close()\n",
    "    l.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64549350-a031-4f96-a7eb-2d33dd218b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(index: int):\n",
    "    plt.title((train_labels[index]))\n",
    "    plt.imshow(train_data[index].reshape(28, 28), cmap=cm.binary)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def check_count_of_each_label():\n",
    "    y_value = np.zeros((1, 10))\n",
    "    for i in range(10):\n",
    "        print(\"Occurence of \", i, \"=\", np.count_nonzero(train_labels == i))\n",
    "        y_value[0, i-1] = np.count_nonzero(train_labels == i)\n",
    "        y_value = y_value.ravel()\n",
    "        x_value = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        plt.xlabel('label')\n",
    "        plt.ylabel('count')\n",
    "        plt.bar(x_value, y_value, 0.7, color='g')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a13de3-5a6e-404a-aaea-90d58dd4b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "def softmax(Z):\n",
    "    e_x = np.exp(Z)\n",
    "    A= e_x / np.sum(np.exp(Z))  \n",
    "    cache=Z\n",
    "    return A,cache\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2409f49c-e082-4e06-8657-6beeb3916821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Z, cache):\n",
    "    Z = cache\n",
    "    length = 10\n",
    "    dZ = np.zeros((60000, 10))\n",
    "    Z = np.transpose(Z)\n",
    "    for row in range(0, 42000):\n",
    "        den = (np.sum(np.exp(Z[row, :])))*(np.sum(np.exp(Z[row, :])))\n",
    "        for col in range(0, 10):\n",
    "            sums = 0\n",
    "            for j in range(0, 10):\n",
    "                if (j != col):\n",
    "                    sums = sums+(math.exp(Z[row, j]))\n",
    "\n",
    "            dZ[row, col] = (math.exp(Z[row, col])*sums)/den\n",
    "    dZ = np.transpose(dZ)\n",
    "    Z = np.transpose(Z)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0816692b-28f1-4e97-bfd3-041b9cd0cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    # np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.zeros((layer_dims[l],\n",
    "                                            layer_dims[l-1])) \n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    print('Shape of initial params', parameters['W1'].shape, parameters['b1'].shape, parameters['W2'].shape, parameters['b2'].shape)\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92a8e6dc-dea3-4068-b65d-940fad0f421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "076324bd-af86-4454-b3ec-e21ceed245c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        # print(\"Z=\"+str(Z))\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78a3dc13-134b-4951-af42-ed2fc91994fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    # number of layers in the neural network\n",
    "    L = len(parameters) // 2\n",
    "    #print(\"In \", L_model_forward.__name__, 'a.shape', A.shape, 'L:' , L)\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(\n",
    "            A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    AL, cache = linear_activation_forward(\n",
    "        A, parameters['W' + str(L)], parameters['b' + str(L)], activation=\"softmax\")\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "037a6842-62f3-4ae4-aeb0-fb596ad3fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "    # print(\"cost=\"+str(cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f223ff-53c5-41ca-9a10-04878585211c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        # dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)  # the number of layers\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    M = len(layers_dims)\n",
    "    current_cache = caches[M-2]\n",
    "    grads[\"dA\"+str(M-1)], grads[\"dW\"+str(M-1)], grads[\"db\"+str(M-1)\n",
    "                                                      ] = linear_activation_backward(dAL, current_cache, activation=\"softmax\")  # M-1\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
    "            grads[\"dA\" + str(l + 2)], current_cache, activation=\"sigmoid\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a410c972-fb66-4a2e-a957-5c3b0de66b8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# upgrade function for weights and bias\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    for l in range(len_update-1):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n",
    "            (learning_rate*grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n",
    "            (learning_rate*grads[\"db\" + str(l+1)])\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5caf9985-0f04-402f-bf97-e96f8f1b0bfd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_graph(cost_plot):\n",
    "\n",
    "    x_value = list(range(1, len(cost_plot)+1))\n",
    "    # print(x_value)\n",
    "    # print(cost_plot)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('cost')\n",
    "    plt.plot(x_value, cost_plot, 0., color='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b9f027e-4c67-4640-b3c5-7543aaec6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, print_cost=False):  # lr was 0.009\n",
    "    print(\"training began...\")\n",
    "    costs = []\n",
    "    cost_plot = np.zeros(num_iterations)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        #print(\"Iteration\", i , \"began\")\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        cost_plot[i] = cost\n",
    "        #print(\"Iteration\", i , \"completed having cost\", cost)\n",
    "\n",
    "    # plot_graph(cost_plot)\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "961dbf88-d2fe-414e-9156-4eeffc8be937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape=(784, 60000)\n",
      "train_label shape=(10, 60000)\n",
      "training began...\n",
      "Shape of initial params (300, 784) (300, 1) (10, 300) (10, 1)\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.MNIST(\n",
    "    root='./data', download=True)\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "if os.path.exists('./data/MNIST/raw/mnist_train.csv'):\n",
    "    train = pd.read_csv(\"./data/MNIST/raw/mnist_train.csv\")\n",
    "else:\n",
    "    convert(\"./data/MNIST/raw/train-images-idx3-ubyte\", \"./data/MNIST/raw/train-labels-idx1-ubyte\",\n",
    "            \"./data/MNIST/raw/mnist_train.csv\", 60000)\n",
    "    train = pd.read_csv(\"./data/MNIST/raw/mnist_train.csv\")\n",
    "\n",
    "if os.path.exists('./data/MNIST/raw/mnist_test.csv'):\n",
    "    test = pd.read_csv(\"./data/MNIST/raw/mnist_test.csv\")\n",
    "else:\n",
    "    convert(\"./data/MNIST/raw/t10k-images-idx3-ubyte\", \"./data/MNIST/raw/t10k-labels-idx1-ubyte\",\n",
    "            \"./data/MNIST/raw/mnist_test.csv\", 10000)\n",
    "    test = pd.read_csv(\"./data/MNIST/raw/mnist_test.csv\")\n",
    "train_labels = np.array(train.loc[:, 'label'])\n",
    "train_data = np.array(train.loc[:, train.columns != 'label'])\n",
    "# visualize(0)\n",
    "# check_count_of_each_label(train_labels)\n",
    "\n",
    "# d = train_data.shape[1]\n",
    "# d1 = 300\n",
    "# # Shape of W1 is given by d1 * d where d1 is 300 and d is given by 784\n",
    "# W1 = np.zeros((d1, d))\n",
    "\n",
    "# # print(W1.shape)\n",
    "# x1 = train_data[0]\n",
    "# # print(x1.shape, x1)\n",
    "# z1 = np.dot(W1, x1)\n",
    "# # print(z1.shape, z1)\n",
    "# a1 = sigmoid(z1)\n",
    "# print('After sigmmoid activation shape is', a1.shape)\n",
    "\n",
    "# W2 = np.zeros((10, d1))\n",
    "# z2 = np.dot(W2, a1)\n",
    "# # print(z2, z2.shape)\n",
    "# y_pred = softmax(z2)\n",
    "# # print(y_pred.shape)\n",
    "\n",
    "# y_actual = train_labels[0]\n",
    "# one_hot = np.zeros(10)\n",
    "# one_hot[y_actual] = 1\n",
    "# print(y_pred, one_hot)\n",
    "# loss = - np.dot(one_hot, np.log(y_pred))\n",
    "# print(loss)\n",
    "\n",
    "###############################\n",
    "\n",
    "train_data = np.reshape(train_data, [784, 60000])\n",
    "train_label = np.zeros((10, 60000))\n",
    "for col in range(60000):\n",
    "    val = train_labels[col]\n",
    "    for row in range(10):\n",
    "        if (val == row):\n",
    "            train_label[val, col] = 1\n",
    "print(\"train_data shape=\"+str(np.shape(train_data)))\n",
    "print(\"train_label shape=\"+str(np.shape(train_label)))\n",
    "\n",
    "# n-layer model (n=3 including input and output layer)\n",
    "layers_dims = [784, 300, 10]\n",
    "len_update = len(layers_dims)\n",
    "parameters = L_layer_model(train_data, train_label, layers_dims,\n",
    "                           learning_rate=0.0005, num_iterations=25, print_cost=True)\n",
    "print(\"training done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ee0479a-6557-4613-b823-531104440c04",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3921612747.py, line 89)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[47], line 89\u001b[0;36m\u001b[0m\n\u001b[0;31m    w1 = np.random.randn(n_hidden1, n_input) * np.sqrt(2/n_input)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the derivative of the sigmoid activation function\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Define the softmax activation function\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    return -np.sum(y * np.log(y_hat))\n",
    "\n",
    "# Define the derivative of the cross-entropy loss function\n",
    "def cross_entropy_loss_prime(y, y_hat):\n",
    "    return y_hat - y\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    y_true = np.argmax(y_true, axis=0)\n",
    "    y_pred = np.argmax(y_pred, axis=0)\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Define the forward propagation function\n",
    "def forward_propagation(x, w1, w2):\n",
    "    z1 = np.dot(w1, x)\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(w2, a1)\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2\n",
    "# Define the backpropagation function\n",
    "def backprop(x, y, w1, w2, learning_rate):\n",
    "    # Forward pass\n",
    "    z1, a1, z2, a2 = forward_propagation(x, w1, w2)\n",
    "    \n",
    "    # Compute the gradients of the loss with respect to the output\n",
    "    #dL_da3 = (a3 - y) / y.shape[1]\n",
    "    \n",
    "    dL_da2 = (a2-y)/y.shape[1]\n",
    "    dz2 = dL_da2\n",
    "    dw2 = np.dot(dz2, a1.T)\n",
    "    da1 = np.dot(w2.T, dz2)\n",
    "    dz1 = da1 * sigmoid_prime(z1)\n",
    "    dw1 = np.dot(dz1, x.T)\n",
    "    # Backward pass\n",
    "    # dz3 = dL_da3\n",
    "    # dw3 = np.dot(dz3, a2.T)\n",
    "    # da2 = np.dot(w3.T, dz3)\n",
    "    # dz2 = da2 * sigmoid_prime(z2)\n",
    "    # dw2 = np.dot(dz2, a1.T)\n",
    "    # da1 = np.dot(w2.T, dz2)\n",
    "    # dz1 = da1 * sigmoid_prime(z1)\n",
    "    # dw1 = np.dot(dz1, x.T)\n",
    "    \n",
    "    # Update the weights and biases\n",
    "    w1 -= learning_rate * dw1\n",
    "    w2 -= learning_rate * dw2\n",
    "    #w3 -= learning_rate * dw3\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = cross_entropy_loss(y, a2)\n",
    "    accuracy = compute_accuracy(y, a2)\n",
    "    \n",
    "    return w1, w2, loss, accuracy\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the input data\n",
    "x_train = x_train.reshape((x_train.shape[0], -1)).T / 255.0\n",
    "x_test = x_test.reshape((x_test.shape[0], -1)).T / 255.0\n",
    "\n",
    "# Convert the target output to one-hot encoding\n",
    "y_train_onehot = np.eye(10)[y_train].T\n",
    "y_test_onehot = np.eye(10)[y_test].T\n",
    "\n",
    "# Define the neural network architecture\n",
    "n_input = 784\n",
    "n_hidden1 = 300\n",
    "n_output = 10\n",
    "\n",
    "# Initialize the weights by sampling from normal distribution\n",
    "w1 = np.random.randn(n_hidden1, n_input) * np.sqrt(2/n_input)\n",
    "w2 = np.random.randn(n_output, n_hidden1) * np.sqrt(2/n_hidden1)\n",
    "#w1 = np.zeros((n_hidden1, n_input)) \n",
    "#w2 = np.zeros((n_output, n_hidden1)) \n",
    "#w1 =  np.random.uniform(-1, 1, (n_hidden1, n_input))\n",
    "#w2 =  np.random.uniform(-1, 1, (n_output, n_hidden1))\n",
    "\n",
    "\n",
    "# Set the hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 35\n",
    "batch_size = 32\n",
    "\n",
    "train_loss = []\n",
    "# Train the neural network using mini-batch gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the training data\n",
    "    indices = np.random.permutation(x_train.shape[1])\n",
    "    x_train = x_train[:, indices]\n",
    "    y_train_onehot = y_train_onehot[:, indices]\n",
    "\n",
    "    # Split the training data into mini-batches\n",
    "    for i in range(0, x_train.shape[1], batch_size):\n",
    "        # Select a mini-batch of input data and corresponding target output\n",
    "        x_batch = x_train[:, i:i+batch_size]\n",
    "        y_batch = y_train_onehot[:, i:i+batch_size]\n",
    "        \n",
    "        # Perform forward pass and backpropagation to update the weights and biases\n",
    "        w1, w2, loss, accuracy = backprop(x_batch, y_batch, w1, w2, learning_rate)\n",
    "        \n",
    "        # Print the loss and accuracy every 10 mini-batches\n",
    "        #if i % (10 * batch_size) == 0:\n",
    "            #print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i//batch_size+1}/{x_train.shape[1]//batch_size}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    train_loss.append(loss)\n",
    "epochs = np.arange(1,num_epochs+1)\n",
    "plt.plot(epochs, train_loss, label='train loss')\n",
    "plt.legend()\n",
    "plt.title(' MNIST learning curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('mlp_scratch.pdf')\n",
    "plt.show()\n",
    "\n",
    "# see how model performs on test data\n",
    "z1, a1, z2, a2 = forward_propagation(x_test, w1, w2)\n",
    "accuracy = compute_accuracy(y_test_onehot, a2)\n",
    "error = 1 - accuracy\n",
    "error, accuracy\n",
    "#(0.033299999999999996, 0.9667)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ce0b1-e58a-40b7-b171-6e4fe231ec02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
